{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "K-Nearest Neighbors implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = open('pa1train.txt', 'r')\n",
    "X_train = []\n",
    "y_train = []\n",
    "for line in train.readlines():\n",
    "    vec = line.split(' ')\n",
    "    X_train.append(vec[:-1])\n",
    "    y_train.append(vec[-1][0])\n",
    "    \n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = open('pa1validate.txt', 'r')\n",
    "X_val = []\n",
    "y_val = []\n",
    "for line in val.readlines():\n",
    "    vec = line.split(' ')\n",
    "    X_val.append(vec[:-1])\n",
    "    y_val.append(vec[-1][0])\n",
    "    \n",
    "X_val = pd.DataFrame(X_val)\n",
    "y_val = pd.DataFrame(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open('pa1test.txt', 'r')\n",
    "X_test = []\n",
    "y_test = []\n",
    "for line in test.readlines():\n",
    "    vec = line.split(' ')\n",
    "    X_test.append(vec[:-1])\n",
    "    y_test.append(vec[-1][0])\n",
    "    \n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucdist(v1, v2):\n",
    "    return np.linalg.norm(v1 - v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_train.columns:\n",
    "    X_train[i] = X_train[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_val.columns:\n",
    "    X_val[i] = X_val[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_test.columns:\n",
    "    X_test[i] = X_test[i].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "y_train = y_train.iloc[:, 0].to_numpy()\n",
    "y_val = y_val.iloc[:, 0].to_numpy()\n",
    "y_test = y_test.iloc[:, 0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NNs\n",
    "def NNs_pred(training_set, labels, x, k):\n",
    "    preds = []\n",
    "    for i in range(len(x)):\n",
    "        distances = []\n",
    "        for j in range(len(training_set)):\n",
    "            distances.append((labels[j], eucdist(x[i], training_set[j])))\n",
    "        distances.sort(key=lambda item: item[1])\n",
    "        distances = distances[:k]\n",
    "        labs = [x[0] for x in distances]\n",
    "        preds.append(mode(labs)[0][0])\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.0, 3: 0.0435, 5: 0.0565, 9: 0.0685, 15: 0.0925}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Error Calculation\n",
    "training_errors = {}\n",
    "for i in [1,3,5,9,15]:\n",
    "    training_errors[i] = np.mean(NNs_pred(X_train, y_train, X_train, i)!=y_train)\n",
    "    print(i)\n",
    "training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.082, 3: 0.098, 5: 0.095, 9: 0.104, 15: 0.108}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation Error Calculation\n",
    "validation_errors = {}\n",
    "for i in [1,3,5,9,15]:\n",
    "    validation_errors[i] = np.mean(NNs_pred(X_train, y_train, X_val, i)!=y_val)\n",
    "    print(i)\n",
    "validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.094, 3: 0.092, 5: 0.098, 9: 0.101, 15: 0.114}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Error Calculation\n",
    "test_errors = {}\n",
    "for i in [1,3,5,9,15]:\n",
    "    test_errors[i] = np.mean(NNs_pred(X_train, y_train, X_test, i)!=y_test)\n",
    "    print(i)\n",
    "test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier with the lowest validation error would be acheived by using the k = 1 parameter. This would mean that the test error of using this parameter is 9.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Projecting data onto a projection matrix (feature reduction) for quicker model execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = open('projection.txt', 'r')\n",
    "proj = []\n",
    "for line in projections.readlines():\n",
    "    vec = line.split(' ')\n",
    "    vec[-1] = vec[-1][:-1]\n",
    "    vec = np.array(vec).astype(float)\n",
    "    proj.append(vec)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_proj = np.matmul(X_train, proj)\n",
    "val_proj = np.matmul(X_val, proj)\n",
    "test_proj = np.matmul(X_test, proj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.0, 3: 0.1605, 5: 0.1945, 9: 0.2305, 15: 0.257}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Error Calculation\n",
    "training_errors = {}\n",
    "for i in [1,3,5,9,15]:\n",
    "    training_errors[i] = np.mean(NNs_pred(train_proj, y_train, train_proj, i)!=y_train)\n",
    "    print(i)\n",
    "training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.32, 3: 0.31, 5: 0.299, 9: 0.302, 15: 0.289}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation Error Calculation\n",
    "validation_errors = {}\n",
    "for i in [1,3,5,9,15]:\n",
    "    validation_errors[i] = np.mean(NNs_pred(train_proj, y_train, val_proj, i)!=y_val)\n",
    "    print(i)\n",
    "validation_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "9\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.314, 3: 0.297, 5: 0.301, 9: 0.293, 15: 0.296}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Error Calculation\n",
    "test_errors = {}\n",
    "for i in [1,3,5,9,15]:\n",
    "    test_errors[i] = np.mean(NNs_pred(train_proj, y_train, test_proj, i)!=y_test)\n",
    "    print(i)\n",
    "test_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classification accuracy of the projected matrix is significantly lower than without projecting each vector. The benefits to projecting each vector is that the runtime is significantly lower however the accuracy has significantly decreased, which makes this a tradeoff between runtime and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
